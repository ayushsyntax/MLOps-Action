{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDh7VRIfd8H2uWCeOwwowW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## End-to-End MLOps Pipeline for Spam Detection\n",
        "\n",
        "**Using Modular Python, DVC, DVC Live, and AWS S3**\n",
        "\n",
        "![Image](https://ml-ops.org/img/mlops-phasen.jpg)\n",
        "\n",
        "![Image](https://mlops-guide.github.io/assets/dvc/dvc_diagram.png)\n",
        "\n",
        "![Image](https://media.geeksforgeeks.org/wp-content/uploads/20250526123213427120/MlOps-Lifecycle_.webp)\n",
        "\n",
        "This project builds a **production-grade MLOps pipeline** for a Spam Detection use case. It transitions from an experimental, notebook-based workflow to a **fully automated, versioned, and reproducible pipeline** using **DVC** and **AWS S3**.\n",
        "\n",
        "The pipeline is modular, parameterized, experiment-driven, and cloud-backed—mirroring real-world industry MLOps practices.\n",
        "\n",
        "---\n",
        "\n",
        "## Phase 1: Project Initialization & Modular Setup\n",
        "\n",
        "### 1. Repository Setup\n",
        "\n",
        "* Create a new GitHub repository\n",
        "* Clone it locally\n",
        "* All development and experiments happen inside this repository\n",
        "\n",
        "### 2. Modular Code Structure\n",
        "\n",
        "* Replace the monolithic Jupyter Notebook with **modular Python scripts**\n",
        "* Create a `src/` directory containing independent pipeline components\n",
        "* Each script performs **one clearly defined task**\n",
        "\n",
        "Example structure:\n",
        "\n",
        "```\n",
        "src/\n",
        " ├── data_ingestion.py\n",
        " ├── data_preprocessing.py\n",
        " ├── feature_engineering.py\n",
        " ├── model_training.py\n",
        " └── model_evaluation.py\n",
        "```\n",
        "\n",
        "### 3. Logging System\n",
        "\n",
        "* Use Python’s `logging` module\n",
        "* Configure:\n",
        "\n",
        "  * **StreamHandler** → console logs\n",
        "  * **FileHandler** → persistent `.log` files\n",
        "* Support log levels such as `DEBUG`, `INFO`, and `ERROR`\n",
        "\n",
        "This enables:\n",
        "\n",
        "* Debugging during development\n",
        "* Traceability during automated DVC runs\n",
        "\n",
        "### 4. Exception Handling\n",
        "\n",
        "* Wrap all major logic inside `try-except` blocks\n",
        "* Log errors instead of failing silently\n",
        "* Raise exceptions after logging to fail fast when required\n",
        "\n",
        "This ensures the pipeline **fails gracefully and transparently**.\n",
        "\n",
        "---\n",
        "\n",
        "## Phase 2: Modular Pipeline Components\n",
        "\n",
        "![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/0%2Amu59arK69UHW7ler.png)\n",
        "\n",
        "![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/0%2ABgalYghgF7tejjzW.png)\n",
        "\n",
        "![Image](https://www.researchgate.net/publication/344335518/figure/fig1/AS%3A938415151394817%401600747015701/TF-IDF-vectorization-process.ppm)\n",
        "\n",
        "![Image](https://miro.medium.com/1%2Ai0o8mjFfCn-uD79-F1Cqkw.png)\n",
        "\n",
        "Each pipeline stage is implemented as an **independent Python module**.\n",
        "\n",
        "### 1. Data Ingestion\n",
        "\n",
        "* Pull raw data from a source (GitHub URL or AWS S3)\n",
        "* Perform an initial **train–test split**\n",
        "* Save outputs to the `data/raw/` directory\n",
        "\n",
        "### 2. Data Preprocessing\n",
        "\n",
        "* Drop irrelevant columns\n",
        "* Encode target labels (spam → 1, ham → 0)\n",
        "* Remove duplicate records\n",
        "* Clean text (lowercasing, tokenization, stemming)\n",
        "\n",
        "### 3. Feature Engineering\n",
        "\n",
        "* Convert text data into numerical vectors\n",
        "* Use **TF-IDF Vectorization**\n",
        "* Control vocabulary size (e.g., `max_features = 500`)\n",
        "* Persist vectorized outputs\n",
        "\n",
        "### 4. Model Training\n",
        "\n",
        "* Train a **Random Forest Classifier**\n",
        "* Use hyperparameters defined externally\n",
        "* Save the trained model as a `.pkl` file in `models/`\n",
        "\n",
        "### 5. Model Evaluation\n",
        "\n",
        "* Evaluate model performance\n",
        "* Compute metrics:\n",
        "\n",
        "  * Accuracy\n",
        "  * Precision\n",
        "  * Recall\n",
        "* Save metrics to a `metrics.json` file\n",
        "\n",
        "---\n",
        "\n",
        "## Phase 3: Pipeline Automation with DVC (`dvc.yaml`)\n",
        "\n",
        "![Image](https://dagshub.com/docs/feature_guide/assets/pipeline/dag.png)\n",
        "\n",
        "![Image](https://files.realpython.com/media/1_new-dvc_add.e7d290c59325.png)\n",
        "\n",
        "![Image](https://i.sstatic.net/k2JjK.png)\n",
        "\n",
        "Manual script execution is replaced with **DVC pipeline orchestration**.\n",
        "\n",
        "### DVC Pipeline Concept\n",
        "\n",
        "* Each pipeline step becomes a **DVC stage**\n",
        "* Stages define:\n",
        "\n",
        "  * Command (`cmd`)\n",
        "  * Dependencies (`deps`)\n",
        "  * Outputs (`outs`)\n",
        "\n",
        "### Example `dvc.yaml`\n",
        "\n",
        "```yaml\n",
        "stages:\n",
        "  data_ingestion:\n",
        "    cmd: python src/data_ingestion.py\n",
        "    deps:\n",
        "      - src/data_ingestion.py\n",
        "    outs:\n",
        "      - data/raw\n",
        "```\n",
        "\n",
        "### Key Benefits\n",
        "\n",
        "* `dvc repro` runs the entire pipeline\n",
        "* DVC automatically skips stages if nothing changed\n",
        "* Saves time and compute\n",
        "* Enables deterministic reproduction\n",
        "\n",
        "You can visualize dependencies using:\n",
        "\n",
        "```\n",
        "dvc dag\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Phase 4: Parameterization with `params.yaml`\n",
        "\n",
        "![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2AKxY4W7eqFb_kgP8xl6Zc3Q.png)\n",
        "\n",
        "![Image](https://blog.paperspace.com/content/images/size/w750/2019/12/yaml.png)\n",
        "\n",
        "![Image](https://spark.apache.org/docs/latest/img/ml-Pipeline.png)\n",
        "\n",
        "Hard-coding values is removed entirely.\n",
        "\n",
        "### Centralized Configuration\n",
        "\n",
        "All tunable values are stored in `params.yaml`:\n",
        "\n",
        "```yaml\n",
        "data_ingestion:\n",
        "  test_size: 0.2\n",
        "\n",
        "feature_engineering:\n",
        "  max_features: 500\n",
        "\n",
        "model_building:\n",
        "  n_estimators: 100\n",
        "  max_depth: 10\n",
        "```\n",
        "\n",
        "### Parameter Loader Function\n",
        "\n",
        "```python\n",
        "def load_params(params_path: str) -> dict:\n",
        "    try:\n",
        "        with open(params_path, 'r') as file:\n",
        "            params = yaml.safe_load(file)\n",
        "        logger.debug('Parameters retrieved from %s', params_path)\n",
        "        return params\n",
        "    except FileNotFoundError:\n",
        "        logger.error('File not found: %s', params_path)\n",
        "        raise\n",
        "    except yaml.YAMLError as e:\n",
        "        logger.error('YAML error: %s', e)\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        logger.error('Unexpected error: %s', e)\n",
        "        raise\n",
        "```\n",
        "\n",
        "### Usage in Pipeline Components\n",
        "\n",
        "```python\n",
        "params = load_params('params.yaml')\n",
        "test_size = params['data_ingestion']['test_size']\n",
        "max_features = params['feature_engineering']['max_features']\n",
        "model_params = params['model_building']\n",
        "```\n",
        "\n",
        "Any parameter change now **automatically propagates** through the pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "## Phase 5: Experiment Tracking with DVC Live\n",
        "\n",
        "![Image](https://doc.dvc.org/static/c25e0c4a1b71ee27e896e86eca1c70f8/c71fc/dvclive-vscode-compare.png)\n",
        "\n",
        "![Image](https://dagshub.com/blog/content/images/2021/07/Experiment-Tracking-Comparison-1.png)\n",
        "\n",
        "![Image](https://www.inetsoft.com/images/website/hr_employee_attrition_analysis.jpg)\n",
        "\n",
        "### Setup\n",
        "\n",
        "```bash\n",
        "pip install dvclive\n",
        "```\n",
        "\n",
        "### DVC Live Integration\n",
        "\n",
        "```python\n",
        "from dvclive import Live\n",
        "\n",
        "with Live(save_dvc_exp=True) as live:\n",
        "    live.log_metric('accuracy', accuracy)\n",
        "    live.log_metric('precision', precision)\n",
        "    live.log_metric('recall', recall)\n",
        "    live.log_params(params)\n",
        "```\n",
        "\n",
        "### Experiment Workflow\n",
        "\n",
        "* `dvc exp run` → executes an experiment\n",
        "* Each run is tracked automatically\n",
        "* Metrics and parameters are stored per experiment\n",
        "* Compare results using:\n",
        "\n",
        "  * `dvc exp show`\n",
        "  * DVC VS Code extension\n",
        "\n",
        "### Experiment Control\n",
        "\n",
        "* `dvc exp remove <exp-name>` → delete experiment\n",
        "* `dvc exp apply <exp-name>` → restore a previous run\n",
        "\n",
        "This enables **systematic hyperparameter tuning** without manual bookkeeping.\n",
        "\n",
        "---\n",
        "\n",
        "## Phase 6: AWS S3 Integration for Remote Storage\n",
        "\n",
        "![Image](https://d2908q01vomqb2.cloudfront.net/e1822db470e60d090affd0956d743cb0e7cdf113/2023/02/17/Arch_Layout_Disconnect_Replication_Image6.png)\n",
        "\n",
        "![Image](https://assets.datacamp.com/production/repositories/6549/datasets/ef10bf4c21182d40dd2509d3042dca7b3faf8dfc/DVC_remote_config.png)\n",
        "\n",
        "![Image](https://docs.aws.amazon.com/images/IAM/latest/UserGuide/images/intro-diagram%20_policies_800.png)\n",
        "\n",
        "### 1. IAM Configuration\n",
        "\n",
        "* Create an IAM user with **Programmatic Access**\n",
        "* Attach `AdministratorAccess` (development only)\n",
        "* Generate:\n",
        "\n",
        "  * Access Key\n",
        "  * Secret Key\n",
        "\n",
        "### 2. S3 Bucket\n",
        "\n",
        "* Create a bucket (e.g., `dvc-s3-proj`)\n",
        "* This bucket becomes the **DVC remote backend**\n",
        "\n",
        "### 3. DVC Remote Setup\n",
        "\n",
        "```bash\n",
        "aws configure\n",
        "dvc remote add -d storage s3://dvc-s3-proj\n",
        "```\n",
        "\n",
        "### 4. Push Artifacts to Cloud\n",
        "\n",
        "```bash\n",
        "dvc push\n",
        "```\n",
        "\n",
        "This uploads:\n",
        "\n",
        "* Data versions\n",
        "* Models\n",
        "* Pipeline outputs\n",
        "\n",
        "Git tracks metadata, while S3 stores large artifacts.\n",
        "\n",
        "---\n",
        "\n",
        "## Final Result\n",
        "\n",
        "This workflow delivers:\n",
        "\n",
        "* Fully modular ML code\n",
        "* Automated, dependency-aware pipelines\n",
        "* Parameterized experimentation\n",
        "* Experiment tracking with lineage\n",
        "* Cloud-backed data and model versioning\n",
        "* Reproducibility across machines and teams\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L24sGjevlQ-a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rI45N9OflU9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "## Command Reference for the End-to-End MLOps Workflow\n",
        "\n",
        "**(Git + Modular Python + DVC + DVC Live + AWS S3)**\n",
        "\n",
        "![Image](https://towardsdatascience.com/wp-content/uploads/2024/08/1ub_u88a4MB5Uj-9Eb60VNA.jpeg)\n",
        "\n",
        "![Image](https://mlops-guide.github.io/assets/dvc/dvc_diagram.png)\n",
        "\n",
        "![Image](https://miro.medium.com/1%2ARXD8hqmXheaJ8cxOG61XEg.jpeg)\n",
        "\n",
        "This section documents **all commands used throughout the lifecycle** of the Spam Detection MLOps project. Commands are grouped by purpose and follow the actual execution order used in a real pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Git and Environment Setup\n",
        "\n",
        "![Image](https://nvie.com/img/git-model%402x.png)\n",
        "\n",
        "![Image](https://uidaholib.github.io/get-git/images/workflow.png)\n",
        "\n",
        "![Image](https://media.geeksforgeeks.org/wp-content/uploads/20200803190037/WelcomepythonVisualStudioCode03082020185517-660x370.png)\n",
        "\n",
        "These commands establish version control, enable collaboration, and track code evolution.\n",
        "\n",
        "* **`git clone <url>`**\n",
        "  Clones the GitHub repository from the remote server to the local machine.\n",
        "\n",
        "* **`cd <folder_name>`**\n",
        "  Moves into the project directory where all development will occur.\n",
        "\n",
        "* **`code .`**\n",
        "  Opens the project directory in Visual Studio Code.\n",
        "\n",
        "* **`git status`**\n",
        "  Displays the current repository state, including staged, unstaged, and untracked files.\n",
        "\n",
        "* **`git add .`**\n",
        "  Stages all modified and new files for commit.\n",
        "\n",
        "* **`git commit -m \"message\"`**\n",
        "  Records the staged changes locally with a descriptive commit message.\n",
        "\n",
        "* **`git push origin main`**\n",
        "  Pushes committed changes from the local branch to the remote GitHub repository.\n",
        "\n",
        "These commands are executed repeatedly throughout the project as the pipeline evolves.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Python Pipeline Execution (Manual Runs)\n",
        "\n",
        "![Image](https://www.jhkinfotech.com/blog/wp-content/uploads/2025/02/Build-a-Machine-Learning-Pipeline-in-Python-3.jpg?v=1738728817)\n",
        "\n",
        "![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2ASVzzwKUdici7WPT4VtUayg.jpeg)\n",
        "\n",
        "![Image](https://pythonbasics.org/wp-content/uploads/2015/12/start-python-script.png)\n",
        "\n",
        "Before automation, each pipeline component is executed independently to validate correctness.\n",
        "\n",
        "* **`python src/data_ingestion.py`**\n",
        "  Runs the script responsible for loading raw data and performing the initial train-test split.\n",
        "\n",
        "* **`python src/pre_processing.py`**\n",
        "  Executes data cleaning steps such as label encoding, duplicate removal, and text normalization.\n",
        "\n",
        "* **`python src/feature_engineering.py`**\n",
        "  Converts cleaned text into numerical features using TF-IDF vectorization.\n",
        "\n",
        "* **`python src/model_training.py`**\n",
        "  Trains the Random Forest classifier and saves the trained model artifact.\n",
        "\n",
        "* **`python src/model_evaluation.py`**\n",
        "  Computes evaluation metrics (accuracy, precision, recall) and stores them as structured outputs.\n",
        "\n",
        "These commands confirm that **each module works independently** before being wired into DVC.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. DVC Pipeline Management\n",
        "\n",
        "![Image](https://dagshub.com/docs/feature_guide/assets/pipeline/dag.png)\n",
        "\n",
        "![Image](https://christophergs.com/assets/images/dvc_workflow2.png)\n",
        "\n",
        "![Image](https://doc.dvc.org/static/39d86590fa8ead1cd1247c883a8cf2c0/cb690/project-versions.png)\n",
        "\n",
        "DVC replaces manual execution with automated, dependency-aware pipelines.\n",
        "\n",
        "* **`dvc init`**\n",
        "  Initializes DVC in the repository and creates required configuration files.\n",
        "\n",
        "* **`dvc repro`**\n",
        "  Executes the full pipeline defined in `dvc.yaml`.\n",
        "  Only stages with changed dependencies are re-run.\n",
        "\n",
        "* **`dvc dag`**\n",
        "  Visualizes the pipeline as a Directed Acyclic Graph (DAG), showing stage dependencies.\n",
        "\n",
        "* **`dvc commit`**\n",
        "  Manually records changes to data or model outputs when auto-tracking is not used.\n",
        "\n",
        "* **`dvc stage add`**\n",
        "  Creates a new pipeline stage directly from the terminal using:\n",
        "\n",
        "  * `-n` for stage name\n",
        "  * `-d` for dependencies\n",
        "  * `-o` for outputs\n",
        "  * `-p` for parameters\n",
        "\n",
        "Once added, stages are stored in `dvc.yaml` and tracked by Git.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Experiment Tracking with DVC Live\n",
        "\n",
        "![Image](https://doc.dvc.org/vscode-customize-table-9ab6cf66aec3b0adfd695f6d80bb2deb.gif)\n",
        "\n",
        "![Image](https://www.kdnuggets.com/wp-content/uploads/awan_7_best_tools_machine_learning_experiment_tracking_5.png)\n",
        "\n",
        "![Image](https://i0.wp.com/dvc.org/wp-content/uploads/2021/07/hyperparameters-july-website.png?fit=2000%2C1385\\&quality=80\\&ssl=1)\n",
        "\n",
        "These commands enable systematic experimentation and metric comparison.\n",
        "\n",
        "* **`pip install dvclive`**\n",
        "  Installs the DVC Live library used for logging metrics and parameters.\n",
        "\n",
        "* **`dvc exp run`**\n",
        "  Executes the pipeline as an experiment.\n",
        "  Each run records metrics, parameters, and artifacts without affecting the main branch.\n",
        "\n",
        "* **`dvc exp show`**\n",
        "  Displays a comparison table of all experiments, including metrics and parameter values.\n",
        "\n",
        "* **`dvc exp remove <exp_name>`**\n",
        "  Deletes a specific experiment and its associated metadata.\n",
        "\n",
        "* **`dvc exp apply <exp_name>`**\n",
        "  Restores the project (code, data, and parameters) to the state of a selected experiment.\n",
        "\n",
        "This allows controlled hyperparameter tuning with full lineage tracking.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. AWS S3 and Remote Storage Integration\n",
        "\n",
        "![Image](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2018/05/01/Prediction1-1.png)\n",
        "\n",
        "![Image](https://miro.medium.com/v2/resize%3Afit%3A1200/1%2AIE7FM99th8pDlIIKHWM2lg.png)\n",
        "\n",
        "![Image](https://blog.nashtechglobal.com/wp-content/uploads/2025/07/image-28.png)\n",
        "\n",
        "These commands connect the local DVC pipeline to cloud-based storage.\n",
        "\n",
        "* **`pip install dvc[s3]`**\n",
        "  Installs DVC with AWS S3 support.\n",
        "\n",
        "* **`pip install awscli`**\n",
        "  Installs the AWS Command Line Interface.\n",
        "\n",
        "* **`aws configure`**\n",
        "  Configures AWS credentials locally by storing:\n",
        "\n",
        "  * Access Key\n",
        "  * Secret Key\n",
        "  * Default region\n",
        "\n",
        "* **`dvc remote add -d <name> s3://<bucket-name>`**\n",
        "  Registers an S3 bucket as the default DVC remote storage.\n",
        "\n",
        "* **`dvc push`**\n",
        "  Uploads all DVC-tracked data, models, and pipeline outputs to the S3 bucket.\n",
        "\n",
        "Git stores metadata, while S3 stores large artifacts.\n",
        "\n",
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "This command set supports:\n",
        "\n",
        "* Version-controlled ML development\n",
        "* Modular pipeline validation\n",
        "* Automated dependency-based execution\n",
        "* Parameterized experimentation\n",
        "* Cloud-backed data and model versioning\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QCvbsUWXltKC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0g5YvFbHl3tr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}